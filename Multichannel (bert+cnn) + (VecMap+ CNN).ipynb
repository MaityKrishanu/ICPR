{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two channal Model for cyberbully detection Channel1-(BERT_Embeddingv+ CNN),  Channel2 - (VecMap+CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers  as ppb \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from tensorflow.keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from tensorflow.keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "#from tensorflow.keras.models import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "import keras.backend as k\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from smart_open import smart_open\n",
    "import datetime \n",
    "#from keras.utils import multi_gpu_model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Code_mixed_bully_dataset.csv\", sep = ',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax=sns.countplot(data.Bully_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "data.head(5)\n",
    "data['processed_tweets'] = data['processed_tweets'].fillna('').apply(str)\n",
    "df_text_genre = data[['processed_tweets', 'Bully_label']]\n",
    "data.head(7)\n",
    "\n",
    "train, test = train_test_split(df_text_genre, test_size=0.15, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "# Function to calculate accuracy \n",
    "def cal_accuracy(y_test, y_pred): \n",
    "    print(\"\\n\")  \n",
    "    print(\"Confusion Matrix:\\n \",confusion_matrix(y_test, y_pred)) \n",
    "      \n",
    "    print (\"Accuracy :\\n \",accuracy_score(y_test,y_pred)*100) \n",
    "      \n",
    "    print(\"Report : \\n\", classification_report(y_test, y_pred))\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#clean_tweets = copy.deepcopy(tweets)\n",
    "def clean_data(tweet):\n",
    "    noises = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m']\n",
    "\n",
    "    for noise in noises:\n",
    "        tweet = tweet.replace(noise, '')\n",
    "\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "def tokenize(tweet):\n",
    "    lower_tweet = tweet.lower()\n",
    "    return word_tokenize(lower_tweet)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "tqdm.pandas(desc=\"Cleaning Data Phase I...\")\n",
    "train['Text_data'] = train['Text_data'].progress_apply(clean_data)\n",
    "test['Text_data'] = test['Text_data'].progress_apply(clean_data)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
    "clean_tweets_train['tokens'] = train['Text_data'].progress_apply(tokenize)\n",
    "clean_tweets_test['tokens'] = test['Text_data'].progress_apply(tokenize)\n",
    "\n",
    "#train_vector = clean_tweets_train['tokens'].tolist()\n",
    "#test_vector = clean_tweets_test['tokens'].tolist()\n",
    "#train_labels = train['label'].values.tolist()\n",
    "#test_labels = test['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc = \"cc.en.300.bin\"\n",
    "loc = \"/DATA/krishanu_2021cs19/krishanu/nlp/Hindi_English/bilingual_embedding/Hin_Eng_align_RCSLS.txt\"\n",
    "#loc = \"Paper_Mikel Artetxe_Hindi_English_align_supervised.txt\"\n",
    "#loc = \"VecMap_Mikel Artetxe_cc_fasttext_supervised_Hindi_English_align.txt\"\n",
    "#loc = \"cc.hi.300.bin\"\n",
    "#loc = \"/DATA/krishanu_2021cs19/krishanu/nlp/Hindi_English/bilingual_embedding/cc.en.300.bin\"\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(loc, binary=False)\n",
    "from gensim.models.wrappers import FastText\n",
    "#print(\"Hi1\")\n",
    "#model = FastText.load_fasttext_format(loc)\n",
    "#print(\"Hi2\")\n",
    "\n",
    "def get_embedding(word):\n",
    "    try:\n",
    "        embedding=model[word]\n",
    "    except:\n",
    "        embedding=np.zeros((300,))\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = []\n",
    "sentences = list(train['processed_tweets'])\n",
    "for sen in sentences:\n",
    "    x_train2.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = []\n",
    "sentences = list(test['processed_tweets'])\n",
    "for sen in sentences:\n",
    "    x_test2.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(x_train2)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train2)\n",
    "x_test = tokenizer.texts_to_sequences(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 30\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer1.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer1.pickle', 'rb') as handle:\n",
    "    tokenizer2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['ram jane tera']\n",
    "sam1 = tokenizer2.texts_to_sequences(sample)\n",
    "sam1\n",
    "sam2 = pad_sequences(sam1, padding='post', maxlen=maxlen)\n",
    "sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import AveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "import keras.backend as k\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = get_embedding(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "pickle.dump(embedding_matrix, open(\"embedding_matrix1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_url, max_seq_length):\n",
    "     inputs = dict(\n",
    "     input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "  )\n",
    "     \n",
    "       # Define muril layer.\n",
    "     muril_layer = hub.KerasLayer(model_url, trainable=False)\n",
    "     outputs = muril_layer(inputs)\n",
    "     assert 'pooled_output' in outputs\n",
    "     assert 'sequence_output' in outputs\n",
    "     sequence_output=outputs[\"sequence_output\"]\n",
    "     filters = 64\n",
    "     convs1 = []\n",
    "     filter_sizes = [1,2]\n",
    "        \n",
    "     \n",
    "     #pooled_output, sequence_output = muril_layer([input_word_ids, input_mask, input_type_ids])\n",
    "     #clf_output = sequence_output[:, 0, :]\n",
    "     input1 = sequence_output\n",
    "     for fsz in filter_sizes:\n",
    "         l_conv = Conv1D(filters, kernel_size=fsz, activation='relu')(input1)\n",
    "         #l_pool = MaxPooling1D(2)(l_conv)\n",
    "         l_pool = AveragePooling1D(3)(l_conv)\n",
    "         l_pool = Dropout(0.25)(l_pool)\n",
    "         convs1.append(l_pool)\n",
    "     l_merge1 = Concatenate(axis=1)(convs1)\n",
    "     flat1 = Flatten()(l_merge1)\n",
    "     drop1 = Dropout(0.4)(flat1)\n",
    "     dense1 = Dense(60, activation='relu')(drop1)\n",
    "     outputs = Dense(2, activation='softmax')(dense1)\n",
    "     model = tf.keras.Model(inputs=[inputs], outputs=outputs)\n",
    "     # compile\n",
    "     model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "     # summarize\n",
    "     print(model.summary())\n",
    "     #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "     return model, muril_layer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
    "\n",
    "#url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "url = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\"\n",
    "#url = \"https://tfhub.dev/google/MuRIL/1\"\n",
    "max_seq_length = 30\n",
    "model, bert_layer = get_model(model_url= url, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert import bert_tokenization\n",
    "import tokenization as bert_tokenization\n",
    "#import bert_tokenizer as tokenizer\n",
    "import numpy as np\n",
    "\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert import bert_tokenization\n",
    "import tokenization as bert_tokenization\n",
    "#import bert_tokenizer as tokenizer\n",
    "import numpy as np\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    " \n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "    input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
    "    for input_string in input_strings:\n",
    "        # Tokenize input.\n",
    "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        sequence_length = min(len(input_ids), max_seq_length)\n",
    "        # Padding or truncation.\n",
    "        if len(input_ids) >= max_seq_length:\n",
    "          input_ids = input_ids[:max_seq_length]\n",
    "        else:\n",
    "          input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "        input_ids_all.append(input_ids)\n",
    "        input_mask_all.append(input_mask)\n",
    "        input_type_ids_all.append([0] * max_seq_length)\n",
    "    return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = create_input(train.processed_tweets.values, tokenizer, max_seq_length=max_seq_length)\n",
    "test_input = create_input(test.processed_tweets.values, tokenizer, max_seq_length=max_seq_length)\n",
    "train_labels =  keras.utils.to_categorical(train.Bully_label.values, num_classes=2)\n",
    "train_label = train.Bully_label.values\n",
    "test_labels =  keras.utils.to_categorical(test.Bully_label.values, num_classes=2)\n",
    "test_label = test.Bully_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'weight_mbert.h5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(weight_file, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, verbose=1)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, \n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],#, earlystopping],\n",
    "    batch_size=64,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the channel 1 with our finetune\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    filters = 64\n",
    "    convs1 = []\n",
    "    filter_sizes = [1,2]\n",
    "    \n",
    "    #bert_input = Input(shape=(length,768))\n",
    "    input0 = Input(shape=(30,768))\n",
    "    #conv0 = Conv1D(filters=64, kernel_size=1, activation='relu')(input0)#, input_shape=(length,768))\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters, kernel_size=fsz, activation='relu')(input0)\n",
    "        #l_pool = MaxPooling1D(2)(l_conv)\n",
    "        l_pool = AveragePooling1D(3)(l_conv)\n",
    "        l_pool = Dropout(0.25)(l_pool)\n",
    "        convs1.append(l_pool)\n",
    "\n",
    "    l_merge1 = Concatenate(axis=1)(convs1)\n",
    "    #drop0 = Dropout(0.5)(conv0)\n",
    "    #pool0 = MaxPooling1D(pool_size=2)(conv0)\n",
    "    flat0 = Flatten()(l_merge1)  \n",
    "    #channel 2\n",
    "    convs2 = []\n",
    "    inputs1 = Input(shape=(maxlen,))\n",
    "    embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)(inputs1)\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv2 = Conv1D(filters, kernel_size=fsz, activation='relu')(embedding1)\n",
    "        #l_pool2 = MaxPooling1D(2)(l_conv2)\n",
    "        l_pool2 = AveragePooling1D(3)(l_conv2)\n",
    "        l_pool2 = Dropout(0.25)(l_pool2)\n",
    "        convs2.append(l_pool2)\n",
    "    l_merge2 = Concatenate(axis=1)(convs2)\n",
    "    #conv1 = Conv1D(filters=64, kernel_size =1, activation='relu')(embedding1)\n",
    "    #drop1 = Dropout(0.5)(conv1)\n",
    "    #pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(l_merge2)\n",
    "\n",
    "    # merge\n",
    "    #flat4 = Flatten()(pool3)\n",
    "    merged = concatenate([flat0,flat1])\n",
    "    drop4 = Dropout(0.5)(merged)\n",
    "    # interpretation\n",
    "    dense1 = Dense(60, activation='relu')(drop4)\n",
    "    drop5 = Dropout(0.5)(dense1)\n",
    "    #dense2 = Dense(40, activation='relu')(drop5)\n",
    "    outputs = Dense(2, activation='sigmoid')(drop5)\n",
    "    model = Model(inputs=[input0, inputs1], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the channel 2 \n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    filters = 64\n",
    "    convs1 = []\n",
    "    filter_sizes = [1,2]\n",
    "    inputs1 = Input(shape=(maxlen,))\n",
    "    embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)(inputs1)\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv2 = Conv1D(filters, kernel_size=fsz, activation='relu')(embedding1)\n",
    "        #l_pool2 = MaxPooling1D(2)(l_conv2)\n",
    "        l_pool2 = AveragePooling1D(3)(l_conv2)\n",
    "        l_pool2 = Dropout(0.25)(l_pool2)\n",
    "        convs1.append(l_pool2)\n",
    "    l_merge2 = Concatenate(axis=1)(convs1)\n",
    "    #conv1 = Conv1D(filters=64, kernel_size =1, activation='relu')(embedding1)\n",
    "    #drop1 = Dropout(0.5)(conv1)\n",
    "    #pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(l_merge2)\n",
    "\n",
    "    # merge\n",
    "    #flat4 = Flatten()(pool3)\n",
    "    #merged = concatenate([flat0,flat1])\n",
    "    drop4 = Dropout(0.4)(flat1)\n",
    "    # interpretation\n",
    "    dense1 = Dense(60, activation='relu')(drop4)\n",
    "    drop5 = Dropout(0.5)(dense1)\n",
    "    #dense2 = Dense(40, activation='relu')(drop5)\n",
    "    outputs = Dense(2, activation='softmax')(drop5)\n",
    "    model = Model(inputs=[inputs1], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "model = define_model(maxlen, vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, train_labels, epochs=35,validation_split=0.15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for Future Inferences\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "#from keras import model_from_json \n",
    "\n",
    "# opening and store file in a variable\n",
    "\n",
    "json_file = open('model1.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# use Keras model_from_json to make a loaded model\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "\n",
    "loaded_model.load_weights(\"model1.h5\")\n",
    "print(\"Loaded Model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = loaded_model.predict(sam2)\n",
    "y_pred1\n",
    "test_prediction1 = np.argmax(y_pred1, axis=-1)\n",
    "test_prediction1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_prediction = np.argmax(y_pred, axis=-1)\n",
    "cal_accuracy(test_prediction,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two channel model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1 + channel 2\n",
    "    filters = 64\n",
    "    convs1 = []\n",
    "    filter_sizes = [1,2,3]\n",
    "    \n",
    "    #bert_input = Input(shape=(length,768))\n",
    "    input0 = Input(shape=(30,768))\n",
    "    #conv0 = Conv1D(filters=64, kernel_size=1, activation='relu')(input0)#, input_shape=(length,768))\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters, kernel_size=fsz, activation='relu')(input0)\n",
    "        #l_pool = MaxPooling1D(2)(l_conv)\n",
    "        l_pool = AveragePooling1D(3)(l_conv)\n",
    "        l_pool = Dropout(0.25)(l_pool)\n",
    "        convs1.append(l_pool)\n",
    "\n",
    "    l_merge1 = Concatenate(axis=1)(convs1)\n",
    "    #drop0 = Dropout(0.5)(conv0)\n",
    "    #pool0 = MaxPooling1D(pool_size=2)(conv0)\n",
    "    flat0 = Flatten()(l_merge1)  \n",
    "    #channel 2\n",
    "    convs2 = []\n",
    "    inputs1 = Input(shape=(maxlen,))\n",
    "    embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)(inputs1)\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv2 = Conv1D(filters, kernel_size=fsz, activation='relu')(embedding1)\n",
    "        #l_pool2 = MaxPooling1D(2)(l_conv2)\n",
    "        l_pool2 = AveragePooling1D(3)(l_conv2)\n",
    "        l_pool2 = Dropout(0.25)(l_pool2)\n",
    "        convs2.append(l_pool2)\n",
    "    l_merge2 = Concatenate(axis=1)(convs2)\n",
    "    #conv1 = Conv1D(filters=64, kernel_size =1, activation='relu')(embedding1)\n",
    "    #drop1 = Dropout(0.5)(conv1)\n",
    "    #pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(l_merge2)\n",
    "\n",
    "    # merge\n",
    "    #flat4 = Flatten()(pool3)\n",
    "    merged = concatenate([flat0,flat1])\n",
    "    drop4 = Dropout(0.5)(merged)\n",
    "    # interpretation\n",
    "    dense1 = Dense(60, activation='relu')(drop4)\n",
    "    drop5 = Dropout(0.5)(dense1)\n",
    "    #dense2 = Dense(40, activation='relu')(drop5)\n",
    "    outputs = Dense(2, activation='sigmoid')(drop5)\n",
    "    model = Model(inputs=[input0, inputs1], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model1 = define_model(40, vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = define_model2(max_seq_length, vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit([train_embedding, x_train], train_labels, validation_split=0.15, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model_bert_vecmap1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "model2 = models.load_model('model_bert_vecmap1.h5')\n",
    "#model1 = keras.models.load_model('model_bert_vecmap.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.fit([x_train], train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testX = encode_text(tokenizer, test.Text_data.values, length)\n",
    "#print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbedding_matrix = np.random.random(300)\n",
    "#mbedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model1.evaluate([train_embedding,x_train], train_labels, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model1.evaluate([test_embedding,x_test],test_labels, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "y_pred = model2.predict([test_embedding, x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = np.argmax(y_pred, axis=-1)\n",
    "test_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_label, test_prediction)\n",
    "\n",
    "cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "print('Confusion Matrix :')\n",
    "print(cm) \n",
    "print('Accuracy Score :',accuracy_score(test_label, test_prediction)) \n",
    "print('Report : ')\n",
    "print(classification_report(test_label, test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_accuracy(test_label, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO get maximum value as 1 and rest to zero\n",
    "y_pred=pd.DataFrame(y_pred)\n",
    "y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)\n",
    "y_pred=y_pred.iloc[:,:].values\n",
    "\n",
    "y_test=pd.DataFrame(test_labels)\n",
    "y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)\n",
    "y_test=y_test.iloc[:,:].values\n",
    "\n",
    "result=[]\n",
    "for i in range(0,len(y_test)):\n",
    "  for j in range(0,len(y_test[0])):\n",
    "    if(y_test[i][j]==1):\n",
    "      result.append(j)\n",
    "\n",
    "predicted=[]\n",
    "for i in range(0,len(y_pred)):\n",
    "  for j in range(0,len(y_pred[0])):\n",
    "    if(y_pred[i][j]==1):\n",
    "      predicted.append(j)\n",
    "\n",
    "#print(result)\n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(result, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With fine tunning\n",
    "def get_two_channel_model_with_train(model_url, max_seq_length,vocab_size):\n",
    "     input0 = dict(\n",
    "     input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "  )\n",
    "     \n",
    "       # Define muril layer.\n",
    "     muril_layer = hub.KerasLayer(model_url, trainable = False)\n",
    "     outputs = muril_layer(input0)\n",
    "     assert 'pooled_output' in outputs\n",
    "     assert 'sequence_output' in outputs\n",
    "     sequence_output=outputs[\"sequence_output\"]\n",
    "     #pooled_output, sequence_output = muril_layer([input_word_ids, input_mask, input_type_ids])\n",
    "     clf_output = sequence_output\n",
    "     embed_layer = Conv1D(filters=64, kernel_size=2, activation='relu')(clf_output)\n",
    "     net = AveragePooling1D(4)(embed_layer)\n",
    "     #gru = Bidirectional(GRU(64, activation='sigmoid', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(net)\n",
    "     #capsule = Capsule(num_capsule=1 ,dim_capsule=72, routings=1,share_weights=True)(gru)\n",
    "     flat0 = Flatten()(net)\n",
    "     \n",
    "     inputs1 = Input(shape=(maxlen,))\n",
    "     embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length , trainable=False)(inputs1)\n",
    "     #for fsz in filter_sizes:\n",
    "        #l_conv2 = Conv1D(filters, kernel_size=fsz, activation='relu')(embedding1)\n",
    "        #l_pool2 = MaxPooling1D(2)(l_conv2)\n",
    "        #l_pool2 = AveragePooling1D(3)(l_conv2)\n",
    "        #l_pool2 = Dropout(0.25)(l_pool2)\n",
    "        #convs2.append(l_pool2)\n",
    "     #l_merge2 = Concatenate(axis=1)(convs2)\n",
    "     conv1 = Conv1D(filters=64, kernel_size =2, activation='relu')(embedding1)\n",
    "     drop1 = Dropout(0.5)(conv1)\n",
    "     pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "     flat1 = Flatten()(pool1)\n",
    "\n",
    "    # merge\n",
    "    #flat4 = Flatten()(pool3)\n",
    "     merged = concatenate([flat0,flat1])\n",
    "     drop4 = Dropout(0.5)(merged)\n",
    "    # interpretation\n",
    "     dense1 = Dense(60, activation='relu')(drop4)\n",
    "     drop5 = Dropout(0.5)(dense1)\n",
    "     outputs = Dense(2, activation='softmax')(drop5)\n",
    "     model = Model(inputs=[input0, inputs1], outputs=outputs)\n",
    "    # compile\n",
    "     model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "     print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    #return model\n",
    "     #output = Dense(2, activation='softmax')(capsule)\n",
    "     \n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "     #model = tf.keras.Model(inputs=inputs,outputs=output)\n",
    "     #model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "     \n",
    "     #assert 'sequence_output' in outputs\n",
    "     \n",
    "     #assert 'pooled_output' in outputs\n",
    "     \n",
    "     #assert 'default' in outputs\n",
    "     \n",
    "     #return tf.keras.Model(inputs=inputs,outputs=outputs[\"sequence_output\"]), muril_layer\n",
    "     return model, muril_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_channel_model_with_train(model_url, max_seq_length,vocab_size):\n",
    "     input0 = dict(\n",
    "     input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "     input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "  )\n",
    "     \n",
    "       # Define muril layer.\n",
    "     muril_layer = hub.KerasLayer(model_url, trainable = False)\n",
    "     outputs = muril_layer(input0)\n",
    "     assert 'pooled_output' in outputs\n",
    "     assert 'sequence_output' in outputs\n",
    "     sequence_output=outputs[\"sequence_output\"]\n",
    "     #pooled_output, sequence_output = muril_layer([input_word_ids, input_mask, input_type_ids])\n",
    "     clf_output = sequence_output\n",
    "     embed_layer = Conv1D(filters=64, kernel_size=2, activation='relu')(clf_output)\n",
    "     net = AveragePooling1D(4)(embed_layer)\n",
    "     #gru = Bidirectional(GRU(64, activation='sigmoid', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(net)\n",
    "     #capsule = Capsule(num_capsule=1 ,dim_capsule=72, routings=1,share_weights=True)(gru)\n",
    "     flat0 = Flatten()(net)\n",
    "     \n",
    "     #inputs1 = Input(shape=(maxlen,))\n",
    "     #embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length , trainable=False)(inputs1)\n",
    "     #for fsz in filter_sizes:\n",
    "        #l_conv2 = Conv1D(filters, kernel_size=fsz, activation='relu')(embedding1)\n",
    "        #l_pool2 = MaxPooling1D(2)(l_conv2)\n",
    "        #l_pool2 = AveragePooling1D(3)(l_conv2)\n",
    "        #l_pool2 = Dropout(0.25)(l_pool2)\n",
    "        #convs2.append(l_pool2)\n",
    "     #l_merge2 = Concatenate(axis=1)(convs2)\n",
    "     #conv1 = Conv1D(filters=64, kernel_size =2, activation='relu')(embedding1)\n",
    "     #drop1 = Dropout(0.5)(conv1)\n",
    "     #pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "     #flat1 = Flatten()(pool1)\n",
    "\n",
    "    # merge\n",
    "    #flat4 = Flatten()(pool3)\n",
    "     #merged = concatenate([flat0,flat1])\n",
    "     drop4 = Dropout(0.5)(flat0)\n",
    "    # interpretation\n",
    "     dense1 = Dense(60, activation='relu')(drop4)\n",
    "     drop5 = Dropout(0.5)(dense1)\n",
    "     outputs = Dense(2, activation='softmax')(drop5)\n",
    "     model = Model(inputs=[input0], outputs=outputs)\n",
    "    # compile\n",
    "     model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "     print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    #return model\n",
    "     #output = Dense(2, activation='softmax')(capsule)\n",
    "     \n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "     #model = tf.keras.Model(inputs=inputs,outputs=output)\n",
    "     #model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "     \n",
    "     #assert 'sequence_output' in outputs\n",
    "     \n",
    "     #assert 'pooled_output' in outputs\n",
    "     \n",
    "     #assert 'default' in outputs\n",
    "     \n",
    "     #return tf.keras.Model(inputs=inputs,outputs=outputs[\"sequence_output\"]), muril_layer\n",
    "     return model, muril_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
    "#url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "#url = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\"\n",
    "url = \"https://tfhub.dev/google/MuRIL/1\"\n",
    "max_seq_length = 30\n",
    "model4, muril_layer = get_one_channel_model_with_train(\n",
    "    model_url= url, max_seq_length=max_seq_length, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3, muril_layer = get_two_channel_model_with_train(\n",
    "    model_url= url, max_seq_length=max_seq_length, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bert import bert_tokenization\n",
    "import tokenization as bert_tokenization\n",
    "#import bert_tokenizer as tokenizer\n",
    "import numpy as np\n",
    "\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "    input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
    "    for input_string in input_strings:\n",
    "        # Tokenize input.\n",
    "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        sequence_length = min(len(input_ids), max_seq_length)\n",
    "        # Padding or truncation.\n",
    "        if len(input_ids) >= max_seq_length:\n",
    "          input_ids = input_ids[:max_seq_length]\n",
    "        else:\n",
    "          input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "        input_ids_all.append(input_ids)\n",
    "        input_mask_all.append(input_mask)\n",
    "        input_type_ids_all.append([0] * max_seq_length)\n",
    "    return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "bert_train_input = create_input(train.Text_data.values, tokenizer, max_seq_length=max_len)\n",
    "bert_test_input = create_input(test.Text_data.values, tokenizer, max_seq_length=max_len)\n",
    "train_labels =  keras.utils.to_categorical(train.label.values, num_classes=2)\n",
    "test_labels =  keras.utils.to_categorical(test.label.values, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit([bert_train_input], train_labels, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit([bert_train_input, x_train], train_labels, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model3.evaluate([bert_train_input,x_train], train_labels, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model3.evaluate([bert_test_input,x_test],test_labels, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "y_pred = model3.predict([bert_test_input, x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = np.argmax(y_pred, axis=-1)\n",
    "test_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_label, test_prediction)\n",
    "\n",
    "cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "print('Confusion Matrix :')\n",
    "print(cm) \n",
    "print('Accuracy Score :',accuracy_score(test_label, test_prediction)) \n",
    "print('Report : ')\n",
    "print(classification_report(test_label, test_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
